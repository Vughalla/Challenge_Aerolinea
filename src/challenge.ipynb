{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este challenge hice uso de principalmente de dos herramientas para el procesamiento de datos: Pandas y PySpark. Ambas, ampliamente utilizadas en el ámbito de la ingeniería de datos.\n",
    "\n",
    "Pandas por su parte es ideal para conjuntos de datos de tamaño moderado que caben cómodamente en la memoria de un solo nodo. Si bien su tiempo de ejecución es más lento que Pyspark, tiene una gran capacidad para trabajar con datos estructurados de forma intuitiva y realizar tareas de limpieza, transformación y preparación de datos.\n",
    "\n",
    "Por otro lado, PySpark es ideal para trabajar con conjuntos de datos de gran escala. Gracias a su arquitectura distribuida aprovecha todos los nodos y si bien esto hace que consuma más memoria, el tiempo de ejecución es significativamente más bajo que herramientas como Pandas. Permite escalar horizontalmente y tiene gran tolerancia a fallos\n",
    "\n",
    "Por todo esto decidí usar Pandas cuando el tamaño de los datos era manejable en memoria y las operaciones requeridas podían realizarse eficientemente en un solo nodo. Recurrí a PySpark cuando necesitábamos procesar los datos de manera distribuida para obtener resultados en un tiempo mucho menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from q1_memory import *\n",
    "from q2_memory import *\n",
    "from q3_memory import *\n",
    "from q1_time import *\n",
    "from q2_time import *\n",
    "from q3_time import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd().replace('\\\\', '/')\n",
    "file_path = \"input/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso que creí necesario para llevar adelante este desarrollo fue desarrollar un primer release, que contenga los desarrollos de las seis funciones y cuyo output sea por supuesto identico entre funciones análogas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q1_memory = q1_memory(f'{base_path}/{file_path}')\n",
    "print(r_q1_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q1_time = q1_time(f'{base_path}/{file_path}')\n",
    "print(r_q1_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q2_memory = q2_memory(f'{base_path}/{file_path}')\n",
    "print(r_q2_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q2_time = q2_time(f'{base_path}/{file_path}')\n",
    "print(r_q2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q3_memory = q3_memory(f'{base_path}/{file_path}')\n",
    "print(r_q3_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q3_time = q3_time(f'{base_path}/{file_path}')\n",
    "print(r_q3_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN DEL PRIMER RELEASE \n",
    "->\n",
    "Teniendo las seis funciones trabajando correctamente, voy a enfocarme ahora en reducir el uso de memoria de aquellas que lo requieran. Usando la librería memory_profiler pude trackear lo que supe que iba a ocurrir. Hasta este punto los dataframe de pandas se creaban con el siguiente comando: json_df = pd.read_json(file_path, lines=True). El problema con el mismo es que guarda en memoria toda la información contenida en el JSON de una sola vez, aumentando el uso de la misma. Este es el mayor problema de este grupo de funciones. Para resolverlo voy a trabajar con 2 versiones diferentes de cada qn_memory() y analizaré el uso de memoria cargando los datos en chuncks de diferentes tamaños vs el uso de memoria de la funcion previamente escrita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INICIO DEL AUMENTO EN PERFORMANCE DE MEMORIA ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from memory_profiler import profile\n",
    "import os\n",
    "\n",
    "base_path = os.getcwd().replace('\\\\', '/')\n",
    "file_path = \"input/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "\n",
    "@profile\n",
    "def q1_memory_old(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    # Crea un dataframe de pandas a partir del json.\n",
    "    json_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    user_df = json_df['user'].apply(pd.Series)\n",
    "    json_df = pd.concat([json_df, user_df], axis=1)\n",
    "\n",
    "    # Convierte el formato de fecha a YYYY-MM-DD\n",
    "    json_df['date'] = pd.to_datetime(json_df['date']).dt.date\n",
    "\n",
    "    # Agrupa por fecha y filtra todas excepto aquellas con mayor cantidad de tweets / dia.\n",
    "    date_df = json_df.groupby(['date']).size().reset_index(name='count').sort_values(by='count').tail(10)\n",
    "\n",
    "    # Remueve del dataframe original todos los registros donde la fecha sea diferente a aquellas donde hubo más actividad.\n",
    "    json_df = json_df[json_df.date.isin(date_df['date'])]\n",
    "\n",
    "    # Agrupa por fecha y usuario, y cuenta el número de tweets por fecha y usuario\n",
    "    grouped_df = json_df.groupby(['date', 'username']).size().reset_index(name='count')\n",
    "\n",
    "    # Encuentra el usuario con más tweets por fecha\n",
    "    max_tweets_df = grouped_df.loc[grouped_df.groupby('date')['count'].idxmax()]\n",
    "\n",
    "    # Ordena los resultados por fecha filtrando aquellas donde hayan menos actividad.\n",
    "    max_tweets_df = max_tweets_df.sort_values(by='date').tail(10)\n",
    "\n",
    "    # Recopila los resultados\n",
    "    return list(zip(max_tweets_df['date'], max_tweets_df['username']))\n",
    "\n",
    "\n",
    "@profile\n",
    "def q1_memory_dev(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    df_concat = pd.DataFrame(columns=['date', 'username'])\n",
    "    for json_df in pd.read_json(file_path, lines=True, chunksize=100):\n",
    "        # Se extraen los key/value del json anidado y se guarda en un nuevo df.\n",
    "        user_df = json_df['user'].apply(pd.Series)\n",
    "\n",
    "        # Se remueven todas las columnas excepto date para reducir la carga en memoria de la variable.\n",
    "        json_df = json_df['date']\n",
    "\n",
    "        # Se remueven todas las columnas excepto username para reducir la carga en memoria de la variable.\n",
    "        user_df = user_df['username']\n",
    "\n",
    "        # Ambos dataframes se unen nuevamente.\n",
    "        json_df = pd.concat([json_df, user_df], axis=1)\n",
    "\n",
    "        df_concat = pd.concat([df_concat, json_df])\n",
    "\n",
    "    json_df = df_concat\n",
    "\n",
    "    # Se elimina el df para liberar espacio en memoria.\n",
    "    del user_df\n",
    "    del df_concat\n",
    "\n",
    "    # Convierte el formato de fecha a YYYY-MM-DD\n",
    "    json_df['date'] = pd.to_datetime(json_df['date']).dt.date\n",
    "\n",
    "    # Agrupa por fecha y filtra todas excepto aquellas con mayor cantidad de tweets / dia.\n",
    "    date_df = json_df.groupby(['date']).size().reset_index(name='count').sort_values(by='count').tail(10)\n",
    "\n",
    "    # Remueve del dataframe original todos los registros donde la fecha sea diferente a aquellas donde hubo más actividad.\n",
    "    json_df = json_df[json_df.date.isin(date_df['date'])]\n",
    "\n",
    "    # Se elimina el df para liberar espacio en memoria.\n",
    "    del date_df\n",
    "\n",
    "    # Agrupa por fecha y usuario, y cuenta el número de tweets por fecha y usuario\n",
    "    json_df = json_df.groupby(['date', 'username']).size().reset_index(name='count')\n",
    "\n",
    "    # Encuentra el usuario con más tweets por fecha\n",
    "    json_df = json_df.loc[json_df.groupby('date')['count'].idxmax()]\n",
    "\n",
    "    # Ordena los resultados por fecha filtrando aquellas donde hayan menos actividad.\n",
    "    json_df = json_df.sort_values(by='date').tail(10)\n",
    "\n",
    "    # Recopila los resultados\n",
    "    return list(zip(json_df['date'], json_df['username']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = q1_memory_old(f'{base_path}/{file_path}')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de la funcion q1_memory. Version del primer release.\n",
    "\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    11   1437.8 MiB   1437.8 MiB           1   @profile\n",
    "    12                                         def q1_memory_old(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    13                                         \n",
    "    14                                             # Crea un dataframe de pandas a partir del json.\n",
    "    15   2530.1 MiB   1092.2 MiB           1       json_df = pd.read_json(file_path, lines=True)\n",
    "    16                                         \n",
    "    17   2650.9 MiB    120.8 MiB           1       user_df = json_df['user'].apply(pd.Series)\n",
    "    18   2667.8 MiB     16.9 MiB           1       json_df = pd.concat([json_df, user_df], axis=1)\n",
    "    19                                         \n",
    "    20                                             # Convierte el formato de fecha a YYYY-MM-DD\n",
    "    21   2668.0 MiB      0.2 MiB           1       json_df['date'] = pd.to_datetime(json_df['date']).dt.date\n",
    "    22                                         \n",
    "    23                                             # Agrupa por fecha y filtra todas excepto aquellas con mayor cantidad de tweets / dia.\n",
    "    24   2668.0 MiB      0.0 MiB           1       date_df = json_df.groupby(['date']).size().reset_index(name='count').sort_values(by='count').tail(10)\n",
    "    25                                         \n",
    "    26                                             # Remueve del dataframe original todos los registros donde la fecha sea diferente a aquellas donde hubo más actividad.\n",
    "    27   2664.8 MiB     -3.2 MiB           1       json_df = json_df[json_df.date.isin(date_df['date'])]\n",
    "    28                                         \n",
    "    29                                             # Agrupa por fecha y usuario, y cuenta el número de tweets por fecha y usuario\n",
    "    30   2668.1 MiB      3.3 MiB           1       grouped_df = json_df.groupby(['date', 'username']).size().reset_index(name='count')\n",
    "    31                                         \n",
    "...\n",
    "    39   2668.1 MiB      0.0 MiB           1       return list(zip(max_tweets_df['date'], max_tweets_df['username']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = q1_memory_dev(f'{base_path}/{file_path}')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de la funcion q1_memory. Version de desarrollo para aumentar la performance.\n",
    "\n",
    "Chuncksize = 100\n",
    "\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    42    146.7 MiB    146.7 MiB           1   @profile\n",
    "    43                                         def q1_memory_dev(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    44                                         \n",
    "    45    146.7 MiB      0.0 MiB           1       df_concat = pd.DataFrame(columns=['date', 'username'])\n",
    "    46    162.1 MiB   -397.7 MiB        1176       for json_df in pd.read_json(file_path, lines=True, chunksize=100):\n",
    "    47                                                 # Crea un dataframe de pandas a partir del json.\n",
    "    48                                         \n",
    "    49                                                 # Se extraen los key/value del json anidado y se guarda en un nuevo df.\n",
    "    50    162.1 MiB   -418.3 MiB        1175           user_df = json_df['user'].apply(pd.Series)\n",
    "    51                                         \n",
    "    52                                                 # Se remueven todas las columnas excepto date para reducir la carga en memoria de la variable.\n",
    "    53    162.1 MiB   -421.6 MiB        1175           json_df = json_df['date']\n",
    "    54                                         \n",
    "    55                                                 # Se remueven todas las columnas excepto username para reducir la carga en memoria de la variable.\n",
    "    56    162.1 MiB   -423.2 MiB        1175           user_df = user_df['username']\n",
    "    57                                         \n",
    "    58                                                 # Ambos dataframes se unen nuevamente.\n",
    "    59    162.1 MiB   -423.2 MiB        1175           json_df = pd.concat([json_df, user_df], axis=1)\n",
    "    60                                         \n",
    "    61    162.1 MiB   -421.9 MiB        1175           df_concat = pd.concat([df_concat, json_df])\n",
    "    62                                         \n",
    "...\n",
    "    91    159.4 MiB      0.0 MiB           1       return list(zip(json_df['date'], json_df['username']))\n",
    "\n",
    "\n",
    "\n",
    "Chuncksize = 1000\n",
    "\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    42    158.4 MiB    158.4 MiB           1   @profile\n",
    "    43                                         def q1_memory_dev(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    44                                         \n",
    "    45    158.4 MiB      0.0 MiB           1       df_concat = pd.DataFrame(columns=['date', 'username'])\n",
    "    46    182.1 MiB   -152.9 MiB         119       for json_df in pd.read_json(file_path, lines=True, chunksize=1000):\n",
    "    47                                                 # Crea un dataframe de pandas a partir del json.\n",
    "    48                                         \n",
    "    49                                                 # Se extraen los key/value del json anidado y se guarda en un nuevo df.\n",
    "    50    182.2 MiB    -12.1 MiB         118           user_df = json_df['user'].apply(pd.Series)\n",
    "    51                                         \n",
    "    52                                                 # Se remueven todas las columnas excepto date para reducir la carga en memoria de la variable.\n",
    "    53    182.2 MiB    -81.6 MiB         118           json_df = json_df['date']\n",
    "    54                                         \n",
    "    55                                                 # Se remueven todas las columnas excepto username para reducir la carga en memoria de la variable.\n",
    "    56    182.2 MiB    -80.1 MiB         118           user_df = user_df['username']\n",
    "    57                                         \n",
    "    58                                                 # Ambos dataframes se unen nuevamente.\n",
    "    59    182.2 MiB    -60.8 MiB         118           json_df = pd.concat([json_df, user_df], axis=1)\n",
    "    60                                         \n",
    "    61    183.1 MiB      0.2 MiB         118           df_concat = pd.concat([df_concat, json_df])\n",
    "    62                                         \n",
    "...\n",
    "    91    179.4 MiB      0.0 MiB           1       return list(zip(json_df['date'], json_df['username']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from memory_profiler import profile\n",
    "import os\n",
    "\n",
    "base_path = os.getcwd().replace('\\\\', '/')\n",
    "file_path = \"input/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "\n",
    "@profile\n",
    "def q2_memory_old(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Lee los datos JSON en un DataFrame de Pandas\n",
    "    json_df = pd.read_json(file_path, lines=True, encoding='utf-8')\n",
    "\n",
    "    # Se filtran del dataframe todas las columnas excepto la deseada para obtener los datos.\n",
    "    json_df = json_df['content'].to_frame() \n",
    "\n",
    "    # Esta expresion regular hace match con todos los codigos Unicode existentes para emojis.\n",
    "    emoji_pattern = r'[\\U0001F000-\\U0001FFFF]'\n",
    "\n",
    "    # Aplica la regex para extraer emojis y crea una nueva columna 'emojis'\n",
    "    json_df['content'] = json_df['content'].str.findall(emoji_pattern)\n",
    "\n",
    "    # Se eliminan los registros cuyo lenght == 0, es decir, aquellos que no contenian emojis.\n",
    "    json_df = json_df[json_df['content'].apply(len) > 0]\n",
    "\n",
    "    # Se aplica un flatten a las listas de registros para que cada registro corresponda a un emoji.\n",
    "    json_df = json_df.explode('content')\n",
    "\n",
    "    # Se agrupan los registros por emoji, contando la frecuencia de cada uno.\n",
    "    json_df = json_df.groupby('content').size().reset_index(name='frequency')\n",
    "\n",
    "    # Ordena los resultados por frecuencia y selecciona los primeros 15 resultados\n",
    "    json_df = json_df.sort_values(by='frequency', ascending=False).head(15)\n",
    "\n",
    "    # Ya que dentro de los codigos Unicode matcheados existían Emoji's Modifiers (modificadores de color por ejemplo) los cuales no son propiamente emojis, se filtran antes de retornar los valores correspondientes.\n",
    "    json_df = json_df[json_df['content'].apply(lambda x: ord(x) not in (45, 65039, 127995, 127997))][0:10]\n",
    "\n",
    "    # Se aplica el formato de salida deseado al dataframe.\n",
    "    return list(zip(json_df['content'], json_df['frequency']))\n",
    "\n",
    "\n",
    "@profile\n",
    "def q2_memory_dev(file_path: str) -> List[Tuple[str, int]]:\n",
    "    df_concat = pd.DataFrame(columns=['content'])\n",
    "\n",
    "    for json_df in pd.read_json(file_path, lines=True, encoding='utf-8', chunksize=100):\n",
    "        # Se filtran del dataframe todas las columnas excepto la deseada para obtener los datos.\n",
    "        json_df = json_df['content'].to_frame() \n",
    "        df_concat = pd.concat([df_concat, json_df])\n",
    "\n",
    "    json_df = df_concat\n",
    "\n",
    "    # Se elimina el df para liberar ese espacio en memoria.\n",
    "    del df_concat\n",
    "\n",
    "    # Esta expresion regular hace match con todos los codigos Unicode existentes para emojis.\n",
    "    emoji_pattern = r'[\\U0001F000-\\U0001FFFF]'\n",
    "\n",
    "    # Aplica la regex para extraer emojis y crea una nueva columna 'emojis'\n",
    "    json_df['content'] = json_df['content'].str.findall(emoji_pattern)\n",
    "\n",
    "    # Se eliminan los registros cuyo lenght == 0, es decir, aquellos que no contenian emojis.\n",
    "    json_df = json_df[json_df['content'].apply(len) > 0]\n",
    "\n",
    "    # Se aplica un flatten a las listas de registros para que cada registro corresponda a un emoji.\n",
    "    json_df = json_df.explode('content')\n",
    "\n",
    "    # Se agrupan los registros por emoji, contando la frecuencia de cada uno.\n",
    "    json_df = json_df.groupby('content').size().reset_index(name='frequency')\n",
    "\n",
    "    # Ordena los resultados por frecuencia y selecciona los primeros 15 resultados\n",
    "    json_df = json_df.sort_values(by='frequency', ascending=False).head(15)\n",
    "\n",
    "    # Ya que dentro de los codigos Unicode matcheados existían Emoji's Modifiers (modificadores de color por ejemplo) los cuales no son propiamente emojis, se filtran antes de retornar los valores correspondientes.\n",
    "    json_df = json_df[json_df['content'].apply(lambda x: ord(x) not in (45, 65039, 127995, 127997))][0:10]\n",
    "\n",
    "    # Se aplica el formato de salida deseado al dataframe.\n",
    "    return list(zip(json_df['content'], json_df['frequency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = q2_memory_old(f'{base_path}/{file_path}')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de la funcion q2_memory. Version del primer release.\n",
    "\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    10   3611.9 MiB   3611.9 MiB           1   @profile\n",
    "    11                                         def q2_memory_old(file_path: str) -> List[Tuple[str, int]]:\n",
    "    12                                             # Lee los datos JSON en un DataFrame de Pandas\n",
    "    13   4691.5 MiB   1079.6 MiB           1       json_df = pd.read_json(file_path, lines=True, encoding='utf-8')\n",
    "    14                                         \n",
    "    15                                             # Se filtran del dataframe todas las columnas excepto la deseada para obtener los datos.\n",
    "    16   4685.3 MiB     -6.3 MiB           1       json_df = json_df['content'].to_frame() \n",
    "    17                                         \n",
    "    18                                             # Esta expresion regular hace match con todos los codigos Unicode existentes para emojis.\n",
    "    19   4685.3 MiB      0.0 MiB           1       emoji_pattern = r'[\\U0001F000-\\U0001FFFF]'\n",
    "    20                                         \n",
    "    21                                             # Aplica la regex para extraer emojis y crea una nueva columna 'emojis'\n",
    "    22   4456.3 MiB   -229.0 MiB           1       json_df['content'] = json_df['content'].str.findall(emoji_pattern)\n",
    "    23                                         \n",
    "    24                                             # Se eliminan los registros cuyo lenght == 0, es decir, aquellos que no contenian emojis.\n",
    "    25   4460.1 MiB      3.8 MiB           1       json_df = json_df[json_df['content'].apply(len) > 0]\n",
    "    26                                         \n",
    "    27                                             # Se aplica un flatten a las listas de registros para que cada registro corresponda a un emoji.\n",
    "    28   4462.8 MiB      2.7 MiB           1       json_df = json_df.explode('content')\n",
    "    29                                         \n",
    "    30                                             # Se agrupan los registros por emoji, contando la frecuencia de cada uno.\n",
    "...\n",
    "    40   4463.1 MiB      0.0 MiB           1       return list(zip(json_df['content'], json_df['frequency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = q2_memory_dev(f'{base_path}/{file_path}')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de la funcion q2_memory. Version de desarrollo para aumentar la performance.\n",
    "\n",
    "\n",
    "Resultados con un chuncksize = 100:\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    43   1375.8 MiB   1375.8 MiB           1   @profile\n",
    "    44                                         def q2_memory_dev(file_path: str) -> List[Tuple[str, int]]:\n",
    "    45   1375.8 MiB      0.0 MiB           1       df_concat = pd.DataFrame(columns=['content'])\n",
    "    46                                         \n",
    "    47   1376.4 MiB -156771.2 MiB        1176       for json_df in pd.read_json(file_path, lines=True, encoding='utf-8', chunksize=100):\n",
    "    48                                                 # Se filtran del dataframe todas las columnas excepto la deseada para obtener los datos.\n",
    "    49   1376.4 MiB -156676.4 MiB        1175           json_df = json_df['content'].to_frame() \n",
    "    50   1376.4 MiB -156615.9 MiB        1175           df_concat = pd.concat([df_concat, json_df])\n",
    "    51                                         \n",
    "    52   1281.0 MiB    -95.4 MiB           1       json_df = df_concat\n",
    "    53                                         \n",
    "    54                                             # Se elimina el df para liberar ese espacio en memoria.\n",
    "    55   1281.0 MiB      0.0 MiB           1       del df_concat\n",
    "    56                                         \n",
    "    57                                             # Esta expresion regular hace match con todos los codigos Unicode existentes para emojis.\n",
    "    58   1281.0 MiB      0.0 MiB           1       emoji_pattern = r'[\\U0001F000-\\U0001FFFF]'\n",
    "    59                                         \n",
    "    60                                             # Aplica la regex para extraer emojis y crea una nueva columna 'emojis'\n",
    "    61   1281.9 MiB      0.9 MiB           1       json_df['content'] = json_df['content'].str.findall(emoji_pattern)\n",
    "    62                                         \n",
    "    63                                             # Se eliminan los registros cuyo lenght == 0, es decir, aquellos que no contenian emojis.\n",
    "...\n",
    "    79   1284.5 MiB      0.0 MiB           1       return list(zip(json_df['content'], json_df['frequency']))\n",
    "\n",
    "\n",
    "\n",
    "Resultados con un chuncksize = 1000:\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    43   1372.7 MiB   1372.7 MiB           1   @profile\n",
    "    44                                         def q2_memory_dev(file_path: str) -> List[Tuple[str, int]]:\n",
    "    45   1372.7 MiB      0.0 MiB           1       df_concat = pd.DataFrame(columns=['content'])\n",
    "    46                                         \n",
    "    47   1401.0 MiB   -322.2 MiB         119       for json_df in pd.read_json(file_path, lines=True, encoding='utf-8', chunksize=1000):\n",
    "    48                                                 # Se filtran del dataframe todas las columnas excepto la deseada para obtener los datos.\n",
    "    49   1401.0 MiB   -326.8 MiB         118           json_df = json_df['content'].to_frame() \n",
    "    50   1401.0 MiB   -292.8 MiB         118           df_concat = pd.concat([df_concat, json_df])\n",
    "    51                                         \n",
    "    52   1401.0 MiB      0.0 MiB           1       json_df = df_concat\n",
    "    53                                         \n",
    "    54                                             # Se elimina el df para liberar ese espacio en memoria.\n",
    "    55   1401.0 MiB      0.0 MiB           1       del df_concat\n",
    "    56                                         \n",
    "    57                                             # Esta expresion regular hace match con todos los codigos Unicode existentes para emojis.\n",
    "    58   1401.0 MiB      0.0 MiB           1       emoji_pattern = r'[\\U0001F000-\\U0001FFFF]'\n",
    "    59                                         \n",
    "    60                                             # Aplica la regex para extraer emojis y crea una nueva columna 'emojis'\n",
    "    61   1401.9 MiB      0.9 MiB           1       json_df['content'] = json_df['content'].str.findall(emoji_pattern)\n",
    "    62                                         \n",
    "    63                                             # Se eliminan los registros cuyo lenght == 0, es decir, aquellos que no contenian emojis.\n",
    "...\n",
    "    79   1405.3 MiB      0.0 MiB           1       return list(zip(json_df['content'], json_df['frequency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from memory_profiler import profile\n",
    "import os\n",
    "\n",
    "base_path = os.getcwd().replace('\\\\', '/')\n",
    "file_path = \"input/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "# Funcion original\n",
    "\n",
    "@profile\n",
    "def q3_memory_old(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Lee los datos JSON y los carga en un DataFrame de Pandas.\n",
    "    json_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Se filtran todas las columnas menos la que contiene la información deseada a la vez que se eliminan los valores nulos de la misma.\n",
    "    json_df = json_df['mentionedUsers'].dropna()\n",
    "\n",
    "    # Se realiza un flatten a la lista de jsons contenida en la columna mentionedUsers y se extraen de la misma solo los username.\n",
    "    json_df = json_df.explode('mentionedUsers').apply(lambda x: x['username']).to_frame() \n",
    "\n",
    "    # Agrupando por nombre de usuario, se realiza un count para saber la cantidad de menciones que cada usuario tuvo en el set de datos.\n",
    "    grouped_df = json_df.groupby(['mentionedUsers']).size().reset_index(name='count')\n",
    "\n",
    "    # Se ordenan los valores en orden descendente y se toman sólo los primeros diez registros, que corresponden a los usuarios más citados.\n",
    "    grouped_df = grouped_df.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "    # Se ordenan los resultados según el formato requerido.\n",
    "    return list(zip(grouped_df['mentionedUsers'], grouped_df['count']))\n",
    "\n",
    "\n",
    "# Nueva version\n",
    "@profile\n",
    "def q3_memory_dev(file_path: str) -> List[Tuple[str, int]]:\n",
    "\n",
    "    df_concat = pd.DataFrame(columns=['mentionedUsers'])\n",
    "    for json_df in pd.read_json(file_path, lines=True, chunksize=100):\n",
    "\n",
    "        #Se filtran todas las columnas menos la que contiene la información deseada a la vez que se eliminan los valores nulos de la misma.\n",
    "        json_df = json_df['mentionedUsers'].dropna()\n",
    "\n",
    "        # Se realiza un flatten a la lista de jsons contenida en la columna mentionedUsers y se extraen de la misma solo los username.\n",
    "        json_df = json_df.explode('mentionedUsers').apply(lambda x: x['username']).to_frame() \n",
    "\n",
    "        df_concat = pd.concat([df_concat, json_df])\n",
    "\n",
    "    # Agrupando por nombre de usuario, se realiza un count para saber la cantidad de menciones que cada usuario tuvo en el set de datos.\n",
    "    grouped_df = df_concat.groupby(['mentionedUsers']).size().reset_index(name='count')\n",
    "\n",
    "    # Se ordenan los valores en orden descendente y se toman sólo los primeros diez registros, que corresponden a los usuarios más citados.\n",
    "    grouped_df = grouped_df.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "    # Se ordenan los resultados según el formato requerido.\n",
    "    return list(zip(grouped_df['mentionedUsers'], grouped_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q3_memory = q3_memory_old(f'{base_path}/{file_path}')\n",
    "print(r_q3_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de la funcion q3_memory. Version del primer release.\n",
    "\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    11    167.8 MiB    167.8 MiB           1   @profile\n",
    "    12                                         def q3_memory_old(file_path: str) -> List[Tuple[str, int]]:\n",
    "    13                                             # Lee los datos JSON y los carga en un DataFrame de Pandas.\n",
    "    14   1406.0 MiB   1238.1 MiB           1       json_df = pd.read_json(file_path, lines=True)\n",
    "    15                                         \n",
    "    16                                             # Se filtran todas las columnas menos la que contiene la información deseada a la vez que se eliminan los valores nulos de la misma.\n",
    "    17   1319.8 MiB    -86.2 MiB           1       json_df = json_df['mentionedUsers'].dropna()\n",
    "    18                                         \n",
    "    19                                             # Se realiza un flatten a la lista de jsons contenida en la columna mentionedUsers y se extraen de la misma solo los username.\n",
    "    20   1321.1 MiB      0.2 MiB      206807       json_df = json_df.explode('mentionedUsers').apply(lambda x: x['username']).to_frame() \n",
    "    21                                         \n",
    "    22                                             # Agrupando por nombre de usuario, se realiza un count para saber la cantidad de menciones que cada usuario tuvo en el set de datos.\n",
    "    23   1323.1 MiB      2.0 MiB           1       grouped_df = json_df.groupby(['mentionedUsers']).size().reset_index(name='count')\n",
    "    24                                         \n",
    "    25                                             # Se ordenan los valores en orden descendente y se toman sólo los primeros diez registros, que corresponden a los usuarios más citados.\n",
    "    26   1323.4 MiB      0.4 MiB           1       grouped_df = grouped_df.sort_values(by='count', ascending=False).head(10)\n",
    "    27                                         \n",
    "    28                                             # Se ordenan los resultados según el formato requerido.\n",
    "    29   1323.4 MiB      0.0 MiB           1       return list(zip(grouped_df['mentionedUsers'], grouped_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q3_memory = q3_memory_dev(f'{base_path}/{file_path}')\n",
    "print(r_q3_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de la funcion q3_memory. Version de desarrollo para aumentar la performance.\n",
    "\n",
    "Resultados con un chuncksize = 100:\n",
    "\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    33    120.2 MiB    120.2 MiB           1   @profile\n",
    "    34                                         def q3_memory_dev(file_path: str) -> List[Tuple[str, int]]:\n",
    "    35                                         \n",
    "    36    120.2 MiB      0.0 MiB           1       df_concat = pd.DataFrame(columns=['mentionedUsers'])\n",
    "    37    132.4 MiB   -372.9 MiB        1176       for json_df in pd.read_json(file_path, lines=True, chunksize=100):\n",
    "    38                                         \n",
    "    39                                                 #Se filtran todas las columnas menos la que contiene la información deseada a la vez que se eliminan los valores nulos de la misma.\n",
    "    40    132.3 MiB   -427.0 MiB        1175           json_df = json_df['mentionedUsers'].dropna()\n",
    "    41                                         \n",
    "    42                                                 # Se realiza un flatten a la lista de jsons contenida en la columna mentionedUsers y se extraen de la misma solo los username.\n",
    "    43    132.3 MiB -78074.3 MiB      207981           json_df = json_df.explode('mentionedUsers').apply(lambda x: x['username']).to_frame() \n",
    "    44                                         \n",
    "    45    132.4 MiB   -452.1 MiB        1175           df_concat = pd.concat([df_concat, json_df])\n",
    "    46                                         \n",
    "    47                                             # Agrupando por nombre de usuario, se realiza un count para saber la cantidad de menciones que cada usuario tuvo en el set de datos.\n",
    "    48    133.2 MiB      0.8 MiB           1       grouped_df = df_concat.groupby(['mentionedUsers']).size().reset_index(name='count')\n",
    "    49                                         \n",
    "    50                                             # Se ordenan los valores en orden descendente y se toman sólo los primeros diez registros, que corresponden a los usuarios más citados.\n",
    "    51    133.2 MiB      0.1 MiB           1       grouped_df = grouped_df.sort_values(by='count', ascending=False).head(10)\n",
    "    52                                         \n",
    "    53                                             # Se ordenan los resultados según el formato requerido.\n",
    "    54    133.2 MiB      0.0 MiB           1       return list(zip(grouped_df['mentionedUsers'], grouped_df['count']))\n",
    "\n",
    "\n",
    "\n",
    "Resultados con un chuncksize = 1000:\n",
    "\n",
    "\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    33    139.0 MiB    139.0 MiB           1   @profile\n",
    "    34                                         def q3_memory_dev(file_path: str) -> List[Tuple[str, int]]:\n",
    "    35                                         \n",
    "    36    139.0 MiB      0.0 MiB           1       df_concat = pd.DataFrame(columns=['mentionedUsers'])\n",
    "    37    150.5 MiB   -148.0 MiB         119       for json_df in pd.read_json(file_path, lines=True, chunksize=1000):\n",
    "    38                                         \n",
    "    39                                                 #Se filtran todas las columnas menos la que contiene la información deseada a la vez que se eliminan los valores nulos de la misma.\n",
    "    40    150.5 MiB   -242.1 MiB         118           json_df = json_df['mentionedUsers'].dropna()\n",
    "    41                                         \n",
    "    42                                                 # Se realiza un flatten a la lista de jsons contenida en la columna mentionedUsers y se extraen de la misma solo los username.\n",
    "    43    150.5 MiB -383061.2 MiB      206924           json_df = json_df.explode('mentionedUsers').apply(lambda x: x['username']).to_frame() \n",
    "    44                                         \n",
    "    45    150.5 MiB   -195.2 MiB         118           df_concat = pd.concat([df_concat, json_df])\n",
    "    46                                         \n",
    "    47                                             # Agrupando por nombre de usuario, se realiza un count para saber la cantidad de menciones que cada usuario tuvo en el set de datos.\n",
    "    48    149.0 MiB     -1.5 MiB           1       grouped_df = df_concat.groupby(['mentionedUsers']).size().reset_index(name='count')\n",
    "    49                                         \n",
    "    50                                             # Se ordenan los valores en orden descendente y se toman sólo los primeros diez registros, que corresponden a los usuarios más citados.\n",
    "    51    149.0 MiB      0.0 MiB           1       grouped_df = grouped_df.sort_values(by='count', ascending=False).head(10)\n",
    "    52                                         \n",
    "    53                                             # Se ordenan los resultados según el formato requerido.\n",
    "    54    149.0 MiB      0.0 MiB           1       return list(zip(grouped_df['mentionedUsers'], grouped_df['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la nueva version del codigo, se logró un importante decremento en el uso de memoria para las tres funciones. Este decremento podría aumentar testeando con diferentes chuncksize evaluando para cada caso cual es el chuck adecuado para cada funcion. Por otra parte, es posible que con algunos ajustes más sobre la estrategia abordada para filtrar y devolver los datos requeridos pueda lograrse un decremento mayor, aunque ya probablemente no tan significativo como lo aquí representado.\n",
    "En cuanto a las funciones que utilizan pyspark me siento satisfecho con la performance alcanzada, procesando 100.000+ registros en un promedio que va entre los 3-5 segundos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
